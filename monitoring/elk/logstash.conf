# =============================================================================
# Logstash Configuration
# =============================================================================
# Pipeline configuration for processing logs from various sources
# =============================================================================

input {
  # Beat input for Filebeat agents
  beats {
    port => 5044
    ssl => false
    client_inactivity_timeout => 300
    codec => json
  }

  # TCP input for direct log shipping
  tcp {
    port => 5000
    codec => json_lines
    tags => ["tcp"]
  }

  # Syslog input
  syslog {
    port => 514
    type => "syslog"
  }

  # HTTP input for webhook-style log ingestion
  http {
    port => 8080
    codec => json
    type => "http"
  }
}

filter {
  # Parse JSON logs
  if [message] =~ /^\{.*\}$/ {
    json {
      source => "message"
      target => "parsed"
      skip_on_invalid_json => true
    }
    
    if [parsed] {
      mutate {
        add_field => {
          "log_level" => "%{[parsed][level]}"
          "service" => "%{[parsed][service]}"
          "trace_id" => "%{[parsed][trace_id]}"
        }
      }
    }
  }

  # Parse Rust backend logs
  if [service] == "backend" or [type] == "backend" {
    grok {
      match => { 
        "message" => [
          "%{TIMESTAMP_ISO8601:timestamp} \[%{LOGLEVEL:level}\] %{DATA:module}: %{GREEDYDATA:log_message}",
          "%{TIMESTAMP_ISO8601:timestamp} %{LOGLEVEL:level} %{GREEDYDATA:log_message}"
        ]
      }
      tag_on_failure => ["_grokparsefailure_backend"]
    }
    
    # Extract trace context if present
    if [log_message] =~ /trace_id/ {
      grok {
        match => { "log_message" => "trace_id=%{UUID:trace_id}" }
        tag_on_failure => []
      }
    }
  }

  # Parse frontend logs
  if [service] == "frontend" or [type] == "frontend" {
    grok {
      match => {
        "message" => [
          "\[%{LOGLEVEL:level}\] %{TIMESTAMP_ISO8601:timestamp} - %{GREEDYDATA:log_message}",
          "%{GREEDYDATA:log_message}"
        ]
      }
      tag_on_failure => ["_grokparsefailure_frontend"]
    }
  }

  # Parse auth-server logs
  if [service] == "auth-server" or [type] == "auth-server" {
    grok {
      match => {
        "message" => "%{TIMESTAMP_ISO8601:timestamp} - %{LOGLEVEL:level}: %{GREEDYDATA:log_message}"
      }
      tag_on_failure => ["_grokparsefailure_auth"]
    }
  }

  # Parse PostgreSQL logs
  if [type] == "postgresql" or [service] == "db" {
    grok {
      match => {
        "message" => "%{TIMESTAMP_ISO8601:timestamp} %{DATA:timezone} \[%{NUMBER:pid}\] %{LOGLEVEL:level}: %{GREEDYDATA:log_message}"
      }
      tag_on_failure => ["_grokparsefailure_postgres"]
    }
  }

  # Parse Nginx access logs
  if [type] == "nginx_access" {
    grok {
      match => { "message" => "%{COMBINEDAPACHELOG}" }
    }
    
    geoip {
      source => "clientip"
      target => "geoip"
    }
    
    useragent {
      source => "agent"
      target => "user_agent"
    }
  }

  # Enrich with environment information
  mutate {
    add_field => {
      "environment" => "${ENVIRONMENT:development}"
      "cluster" => "${CLUSTER_NAME:default}"
    }
  }

  # Standardize timestamp
  date {
    match => ["timestamp", "ISO8601", "yyyy-MM-dd HH:mm:ss,SSS", "yyyy-MM-dd'T'HH:mm:ss.SSS'Z'"]
    target => "@timestamp"
  }

  # Normalize log levels
  mutate {
    gsub => [
      "level", "(?i)warn(ing)?", "WARN",
      "level", "(?i)err(or)?", "ERROR",
      "level", "(?i)info(rmation)?", "INFO",
      "level", "(?i)debug", "DEBUG",
      "level", "(?i)trace", "TRACE"
    ]
  }

  # Remove unnecessary fields
  mutate {
    remove_field => ["host", "agent", "ecs", "input", "tags"]
  }

  # Add metrics for error tracking
  if [level] == "ERROR" {
    metrics {
      meter => "errors"
      add_tag => "metric"
      flush_interval => 60
    }
  }
}

output {
  # Send to Elasticsearch
  elasticsearch {
    hosts => ["${ELASTICSEARCH_HOST:localhost}:${ELASTICSEARCH_PORT:9200}"]
    index => "logs-%{[service]:unknown}-%{+YYYY.MM.dd}"
    user => "${ELASTICSEARCH_USER:elastic}"
    password => "${ELASTICSEARCH_PASSWORD:changeme}"
    
    ilm_enabled => true
    ilm_rollover_alias => "logs"
    ilm_pattern => "000001"
    ilm_policy => "logs-policy"
    
    template_name => "logs"
    template_overwrite => true
  }

  # Debug output (disabled in production)
  if "${DEBUG:false}" == "true" {
    stdout {
      codec => rubydebug
    }
  }

  # Send errors to separate index for monitoring
  if [level] == "ERROR" {
    elasticsearch {
      hosts => ["${ELASTICSEARCH_HOST:localhost}:${ELASTICSEARCH_PORT:9200}"]
      index => "errors-%{+YYYY.MM.dd}"
      user => "${ELASTICSEARCH_USER:elastic}"
      password => "${ELASTICSEARCH_PASSWORD:changeme}"
    }
  }

  # Send metrics to monitoring index
  if "metric" in [tags] {
    elasticsearch {
      hosts => ["${ELASTICSEARCH_HOST:localhost}:${ELASTICSEARCH_PORT:9200}"]
      index => "metrics-%{+YYYY.MM.dd}"
      user => "${ELASTICSEARCH_USER:elastic}"
      password => "${ELASTICSEARCH_PASSWORD:changeme}"
    }
  }
}
